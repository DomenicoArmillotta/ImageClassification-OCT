{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomenicoArmillotta/ImageClassification-OCT/blob/main/OCT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ISSUE (READ BEFORE)** : \n",
        "\n",
        "The main problem encountered, that keras is used for small datasets, is it has much lower performance than tensorflow or pyTorch.\n",
        "This is why the training took too long, so for an implementation of a problem with real objects and images, it is not advisable to use the keras framework.\n",
        "Here you can see the benchmarks between pyTorch and keras (https://wrosinski.github.io/deep-learning-speed-vol1/).\n",
        "So it was decided to switch to pyTorch , to implement models with a framework also used in research and corporate. \n",
        "keras has the characteristic of being easier to use and organise, but given its low performance, being a high-level framework, we were forced to change the framework.\n",
        "That is why the explainability and the various tests of the written architectures were not continued.\n",
        "The report below will describe the work done in the first moments, up to the interruption. The report and all the precautions taken with regard to the code have been scrupulously reviewed, trying to bring the keras framework to its full potential. \n",
        "Work has not been interrupted, but implementations using pyTorch will appear in the near future."
      ],
      "metadata": {
        "id": "-vTUyXcrr01K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGw929d0TdbM"
      },
      "source": [
        "Cose da fare e/o controllare : \n",
        "\n",
        "*  FARE  Test\n",
        "* provare con le pre trainate : inceptionv3 , mobilenet\n",
        "* capire se va meglio dopo aver fatto rescale 1/255\n",
        "\n",
        "https://www.tensorflow.org/tutorials/load_data/images?hl=it\n",
        "\n",
        "**Feature reuse** : \n",
        "This approach is typically used when the data and task being tackled are very different from the original task the pre-trained model was trained on.\n",
        "\n",
        "**Fine-tuning** : \n",
        " This approach is typically used when the data and task being tackled are similar to the original task the pre-trained model was trained on\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6yTVTSqNwje"
      },
      "source": [
        "[Dataset link](https://www.kaggle.com/datasets/paultimothymooney/kermany2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAU-KYUWDH64"
      },
      "source": [
        "**Pay Attention** : Is used an organization of the code made to easy run the code to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FjPYmYBVSJ1"
      },
      "source": [
        "**Resume risultati :**\n",
        "1. Mnist --> 75% ma overfitting    5 epoche\n",
        "2. Alexnet --> 60%     1 epoca\n",
        "3. Efficent net --> 35%   5 epoche\n",
        "4. Inception --> 77%     3.5 epoche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD4i_zUEVX2p"
      },
      "source": [
        "# **Library + Class weight**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn7K-P65Nve3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Flatten\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(42)\n",
        "from google.colab import drive\n",
        "\n",
        "from matplotlib import style\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Input, Activation, add, Add, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras import optimizers\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr9pme01gwwm"
      },
      "source": [
        "Distribution of the class inside the folder , without using np array\n",
        "\n",
        "is pretty fast --> 5s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbbQ4XNhLvsQ",
        "outputId": "0c79450c-dc4c-4f51-e8c5-f81c5ed46232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# setup directory used in the project for training and test phase\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/dataset/OCT/'\n",
        "train_dir = '/content/drive/MyDrive/dataset/OCT/train'\n",
        "test_dir = '/content/drive/MyDrive/dataset/OCT/test'\n",
        "val_dir = '/content/drive/MyDrive/dataset/OCT/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ms6x4iYgoyK"
      },
      "outputs": [],
      "source": [
        "#compute class weight train for inbalanced dataset --> estimated time for 80k documents = 3s\n",
        "\n",
        "import pathlib\n",
        "\n",
        "# create array with all classes in the train\n",
        "summary = pd.DataFrame()\n",
        "val_classes = [item.name for item in \n",
        "                 pathlib.Path('/content/drive/MyDrive/dataset/OCT/train').glob('*')]\n",
        "\n",
        "# count the number of files in each folder\n",
        "# print(val_classes)\n",
        "for d in ['train']:\n",
        "    for c in val_classes:\n",
        "        n = len([i for i in pathlib.Path(f'/content/drive/MyDrive/dataset/OCT/{d}/{c}').glob('*')])\n",
        "        summary.loc[c, d] = n\n",
        "summary.style.format(\"{:.0f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPIp6DKig11l"
      },
      "source": [
        "Compue class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbZJcoKz-p2s"
      },
      "outputs": [],
      "source": [
        "train_directory = '/content/drive/MyDrive/dataset/OCT/train/'\n",
        "\n",
        "# ['DME', 'CNV', 'NORMAL', '.DS_Store', 'DRUSEN']\n",
        "from tqdm import tqdm\n",
        "def get_data(folder):\n",
        "    \"\"\"\n",
        "    Load the data and labels from the given folder.\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    for folderName in os.listdir(folder):\n",
        "        if not folderName.startswith('.'):\n",
        "            if folderName in ['NORMAL']:\n",
        "                label = 0\n",
        "            elif folderName in ['CNV']:\n",
        "                label = 1\n",
        "            elif folderName in ['DME']:\n",
        "                label = 2\n",
        "            elif folderName in ['DRUSEN']:\n",
        "                label = 3\n",
        "            else:\n",
        "                label = 4\n",
        "            for image_filename in tqdm(os.listdir(folder + folderName)):\n",
        "                y.append(label)\n",
        "    y = np.asarray(y)\n",
        "    return y\n",
        "y_train= get_data(train_directory)\n",
        "\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y_trainHot = to_categorical(y_train, num_classes = 4)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weight = class_weight.compute_class_weight(class_weight = \"balanced\", classes = np.unique(y_train),y =  y_train)\n",
        "class_weights = dict(zip(np.unique(y_train), class_weight))\n",
        "class_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6IGQqs9SLQu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initializing a dictionary with values\n",
        "class_weights = {0: '0.7932072962188865', 1: '0.5608977804052239', 2: '1.839376982728234', 3: '2.4226149025069637'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q1SpqzrN7mL"
      },
      "source": [
        "{0: 0.7932072962188865,\n",
        " 1: 0.5608977804052239,\n",
        " 2: 1.839376982728234,\n",
        " 3: 2.4226149025069637}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVgC1tgRxYVC"
      },
      "source": [
        "# **1. ALEXNET from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIHeabC3A73A",
        "outputId": "cb6d69c6-606c-4424-f282-26fd8d190eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# setup directory used in the project for training and test phase\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/dataset/OCT/'\n",
        "train_dir = '/content/drive/MyDrive/dataset/OCT/train'\n",
        "test_dir = '/content/drive/MyDrive/dataset/OCT/test'\n",
        "val_dir = '/content/drive/MyDrive/dataset/OCT/val/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS6wc7JDBByX"
      },
      "outputs": [],
      "source": [
        "train = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='grayscale',  #greyscale = 1 channel image\n",
        "    batch_size=128,          \n",
        "    image_size=(50, 50),     #qui le dim dell'immagine , da esperimenti risulta che se cambiamo a 150 non cambia troppo il tempo di train\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,  #abbiamo gia il validation set\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "val = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='grayscale',\n",
        "    batch_size=128,\n",
        "    image_size=(50, 50),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJrk-LYABHa2"
      },
      "outputs": [],
      "source": [
        "#usato per la normalizzazzione -> lo fa keras negli esempi -> velocizza il training\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)    \n",
        "val = val.map(lambda x, y: (normalization_layer(x), y))\n",
        "train = train.map(lambda x, y: (normalization_layer(x), y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN3xf4gjBJio"
      },
      "outputs": [],
      "source": [
        "# mantiene le immagini in memoria dopo che sono state caricate dal disco durante la prima epoca. Velocizza il training\n",
        "#lo fa keras negli esempi\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val = val.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_yRfRhtxdh8"
      },
      "outputs": [],
      "source": [
        "#Early stopping -> sistemata\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=7,\n",
        "    mode='auto',\n",
        "    restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfc6Lk2exsW5",
        "outputId": "619cab52-147b-4496-cba9-b48a94a89a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 48, 48, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 22, 22, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 22, 22, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 11, 11, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 11, 11, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 9, 9, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 9, 9, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 2, 2, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 2, 2, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 1, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1, 1, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               131584    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 786,052\n",
            "Trainable params: 785,092\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "img_size = 50\n",
        "model = keras.models.Sequential()\n",
        "#1st Convolutional Layer\n",
        "model.add(keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu', input_shape = [img_size, img_size,1]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "#2nd Convolutional Layer\n",
        "model.add(keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "#3rd Convolutional Layer\n",
        "model.add(keras.layers.Conv2D(filters=128,kernel_size=3,activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "#4th Convolutional Layer\n",
        "model.add(keras.layers.Conv2D(filters=256,kernel_size=3,activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "#Passing it to a Fully Connected layer\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# 1st Fully Connected Layer\n",
        "model.add(keras.layers.Dense(512,activation='relu'))\n",
        "\n",
        "# 2st Fully Connected Layer\n",
        "model.add(keras.layers.Dense(512,activation='relu'))\n",
        "\n",
        "#Softmax layer for output\n",
        "model.add(keras.layers.Dense(4,activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz2u4wsGRv6x"
      },
      "source": [
        "### Training **Senza** Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "JIuDgya1x3FN",
        "outputId": "d42b5af5-a5c2-47d7-a76a-f7efe5884a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            " 68/150 [============>.................] - ETA: 36:44 - loss: 1.3959 - accuracy: 0.6013"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e14ed6388b94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train, epochs=12, validation_data=val ,steps_per_epoch=150 , verbose = 1 , callbacks=[early_stopping] , class_weight = class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIQ2A8wsE77V"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/model/AlexNet_noAUG.h5')   # always save your weights after training or during training\n",
        "#evaluation with history\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQduiJm_FGDZ"
      },
      "outputs": [],
      "source": [
        "#disconnessione per non consumare risorse\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOhAx5JqRgm1"
      },
      "source": [
        "### Training **Con** Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_irqhW5Rm50"
      },
      "outputs": [],
      "source": [
        "#Augmentation --> farlo con il layer\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    fill_mode=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DgxvI9CRj9k"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "img_height = 50\n",
        "batch_size=32\n",
        "epochs = 20\n",
        "model.fit(aug.flow_from_directory(directory=train_dir,target_size=(img_height, img_height),batch_size=batch_size ,shuffle=True,color_mode='grayscale',seed=None,class_mode='categorical'),\n",
        "          epochs=epochs,\n",
        "          validation_data=val,\n",
        "          class_weight = class_weights\n",
        "          , callbacks = [early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqKHhndx0lFK"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/model/AlexNet_AUG.h5')   # always save your weights after training or during training\n",
        "#evaluation with history\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_KExmJYCXON"
      },
      "outputs": [],
      "source": [
        "#disconnessione per non consumare risorse\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test**"
      ],
      "metadata": {
        "id": "9PFsgwmnhMoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_dim = 150;\n",
        "batch_size = 64;\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  test_dir,\n",
        "  validation_split=None,\n",
        "  seed=123,\n",
        "  image_size=(img_dim, img_dim),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "fA8q_aKyhPgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_probabilities = model.predict(test_ds)\n",
        "pred=np.argmax(test_probabilities,axis=1)\n",
        "def extract_labels(image, label):\n",
        "    return label\n",
        "\n",
        "labels = test_ds.map(extract_labels).unbatch().batch(batch_size)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = []  # store predicted labels\n",
        "y_true = []  # store true labels\n",
        "\n",
        "# iterate over the dataset\n",
        "for image_batch, label_batch in test_ds:   # use dataset.unbatch() with repeat\n",
        "   # append true labels\n",
        "   y_true.append(label_batch)\n",
        "   # compute predictions\n",
        "   preds = model.predict(image_batch)\n",
        "   # append predicted labels\n",
        "   y_pred.append(np.argmax(preds, axis = - 1))\n",
        "\n",
        "# convert the true and predicted labels into tensors\n",
        "correct_labels = tf.concat([item for item in y_true], axis = 0)\n",
        "predicted_labels = tf.concat([item for item in y_pred], axis = 0)\n",
        "\n",
        "cm = confusion_matrix(correct_labels, predicted_labels)\n",
        "# Plot the confusion matrix using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')"
      ],
      "metadata": {
        "id": "potjiiucj8lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = model.evaluate(test_ds)\n",
        "print('Test Accuracy:', test_accuracy)"
      ],
      "metadata": {
        "id": "aAi7fmvpk1tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Generate the classification report\n",
        "report = classification_report(correct_labels, predicted_labels)\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "id": "VidTaeNCk3Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_-tCbAQ6jrV"
      },
      "source": [
        "# **2. MNIST from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtg5VUABBiXo"
      },
      "outputs": [],
      "source": [
        "# setup directory used in the project for training and test phase\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/dataset/OCT/'\n",
        "train_dir = '/content/drive/MyDrive/dataset/OCT/train'\n",
        "test_dir = '/content/drive/MyDrive/dataset/OCT/test'\n",
        "val_dir = '/content/drive/MyDrive/dataset/OCT/val/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU1WUrZEBjFc"
      },
      "outputs": [],
      "source": [
        "train = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='grayscale',\n",
        "    batch_size=128,\n",
        "    image_size=(50, 50),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "val = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='grayscale',\n",
        "    batch_size=128,\n",
        "    image_size=(50, 50),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDmLMvD2BrGt"
      },
      "outputs": [],
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "val = val.map(lambda x, y: (normalization_layer(x), y))\n",
        "train = train.map(lambda x, y: (normalization_layer(x), y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwuHzOuzBrSr"
      },
      "outputs": [],
      "source": [
        "# mantiene le immagini in memoria dopo che sono state caricate dal disco durante la prima epoca. Velocizza il training\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val = val.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slNS-YM6BrX-"
      },
      "outputs": [],
      "source": [
        "#Early stopping -> sistemata\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=7,\n",
        "    mode='auto',\n",
        "    restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUHXiX3_Brac"
      },
      "outputs": [],
      "source": [
        "#Augmentation --> farlo con il layer\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    fill_mode=\"nearest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Roo_mGsb6m1c",
        "outputId": "9bded1d9-26d2-4dcf-9d31-62fce9645e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 22, 22, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 11, 11, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 9, 9, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               512250    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 1004      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 605,926\n",
            "Trainable params: 605,926\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#we made the model with the augmentation layer\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu', input_shape = [50, 50,1]))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(keras.layers.Conv2D(filters=64 , activation='relu',kernel_size=3))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(keras.layers.Conv2D(filters=128 , activation='relu' ,kernel_size=3))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(250,activation='relu'))\n",
        "model.add(keras.layers.Dense(4,activation='softmax'))\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePPuw6kzCNoI"
      },
      "source": [
        "### Train **senza** augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bmkamEP64ZM",
        "outputId": "39859f35-d9c3-4a09-d013-bb4bd530c360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "150/150 [==============================] - 2311s 13s/step - loss: 1.3722 - accuracy: 0.4941 - val_loss: 1.2377 - val_accuracy: 0.5000\n",
            "Epoch 2/8\n",
            "150/150 [==============================] - 1883s 13s/step - loss: 1.0494 - accuracy: 0.6640 - val_loss: 0.9739 - val_accuracy: 0.5000\n",
            "Epoch 3/8\n",
            "150/150 [==============================] - 2039s 14s/step - loss: 0.9520 - accuracy: 0.6917 - val_loss: 0.8526 - val_accuracy: 0.5625\n",
            "Epoch 4/8\n",
            "150/150 [==============================] - 2298s 15s/step - loss: 0.8734 - accuracy: 0.7266 - val_loss: 0.7811 - val_accuracy: 0.6250\n",
            "Epoch 5/8\n",
            " 53/150 [=========>....................] - ETA: 16:41 - loss: 0.8313 - accuracy: 0.7469"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1200 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r150/150 [==============================] - 550s 4s/step - loss: 0.8313 - accuracy: 0.7469 - val_loss: 0.7957 - val_accuracy: 0.5938\n"
          ]
        }
      ],
      "source": [
        "#train senza augmentation\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "history = model.fit(train, epochs=8, validation_data=val , verbose = 1 , steps_per_epoch=150 ,  class_weight = class_weights , callbacks = [early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "2PN0aLKv7A5b",
        "outputId": "c4f5c172-04d6-424c-8ad5-f965bf8944d5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.save('/content/drive/MyDrive/dataset/model/MNIST.h5')   # always save your weights after training or during training\n",
        "#evaluation with history\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljXgyCt1CvTI"
      },
      "outputs": [],
      "source": [
        "#disconnessione per non consumare risorse\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZoh6L2MUD93"
      },
      "source": [
        "# **3. EFFICENT NET pre trained**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaGLzHBhLszy",
        "outputId": "b5c3ca01-bd4c-4ff4-c2d8-2175ac9636ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# setup directory used in the project for training and test phase\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/dataset/OCT/'\n",
        "train_dir = '/content/drive/MyDrive/dataset/OCT/train'\n",
        "test_dir = '/content/drive/MyDrive/dataset/OCT/test'\n",
        "val_dir = '/content/drive/MyDrive/dataset/OCT/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "yuzuzRg4YDJu",
        "outputId": "06660d4f-556b-4164-e723-3c7f5d815c1d"
      },
      "outputs": [
        {
          "ename": "UnknownError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4f86d507251f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inferred'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0mlabels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mpartial_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0mlabels_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mfilenames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpartial_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_subdirectory\u001b[0;34m(directory, class_indices, follow_links, formats)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mabsolute_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/dataset_utils.py\u001b[0m in \u001b[0;36miter_valid_files\u001b[0;34m(directory, follow_links, formats)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mwalk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mwalk_v2\u001b[0;34m(top, topdown, onerror)\u001b[0m\n\u001b[1;32m    863\u001b[0m   \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m     \u001b[0mlisting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m   \"\"\"\n\u001b[0;32m--> 748\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlist_directory_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    775\u001b[0m   return [\n\u001b[1;32m    776\u001b[0m       \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pywrap_file_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetChildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m   ]\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: /content/drive/MyDrive/dataset/OCT/train/CNV; Input/output error"
          ]
        }
      ],
      "source": [
        "#Augmentation --> farlo con il layer\n",
        "\n",
        "m_channels = 3\n",
        "train = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb', #--> attenzione a questo valore , con efficentnet per forza rgb\n",
        "    batch_size=64,\n",
        "    image_size=(150, 150),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n",
        "val = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb', #-> il modello non accetta 1 canale \n",
        "    batch_size=64,\n",
        "    image_size=(150, 150),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n",
        "# PER FARE L'AUGMENTATION\n",
        "'''\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    fill_mode=\"nearest\")\n",
        "\n",
        "img_dim = 150\n",
        "batch_size=64\n",
        "train_augmented = aug.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(img_dim, img_dim),\n",
        "    batch_size=batch_size ,\n",
        "    shuffle=True,\n",
        "    color_mode='rgb',\n",
        "    seed=None,\n",
        "    class_mode='categorical')\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZi6tFrjRaTB"
      },
      "outputs": [],
      "source": [
        "#usato per la normalizzazzione -> lo fa keras negli esempi -> velocizza il training\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)    \n",
        "val = val.map(lambda x, y: (normalization_layer(x), y))\n",
        "train = train.map(lambda x, y: (normalization_layer(x), y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFPhJTE-RaXt"
      },
      "outputs": [],
      "source": [
        "# mantiene le immagini in memoria dopo che sono state caricate dal disco durante la prima epoca. Velocizza il training\n",
        "#lo fa keras negli esempi\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val = val.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mFzvW2cckSg"
      },
      "outputs": [],
      "source": [
        "# Create Model\n",
        "img_dim = 150\n",
        "m_channels = 3\n",
        "model_ENB0 = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(img_dim,img_dim,m_channels))\n",
        "model_ENB0.trainable = False\n",
        "CLASSES = 4\n",
        "model = Sequential()\n",
        "model.add(model_ENB0)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(CLASSES,activation=\"softmax\"))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZLNxjXWDH3J"
      },
      "source": [
        "### **OPTION 1 = Feature Reuse Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "971QXTw0dcXZ",
        "outputId": "1a8c135a-8056-4315-ff5d-ca1d06a30adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "150/150 [==============================] - 1155s 7s/step - loss: 1.6340 - accuracy: 0.3260 - val_loss: 1.6969 - val_accuracy: 0.2500\n",
            "Epoch 2/15\n",
            "150/150 [==============================] - 1062s 7s/step - loss: 1.6289 - accuracy: 0.3270 - val_loss: 1.7872 - val_accuracy: 0.2500\n",
            "Epoch 3/15\n",
            "150/150 [==============================] - 1060s 7s/step - loss: 1.6124 - accuracy: 0.3326 - val_loss: 1.6753 - val_accuracy: 0.2500\n",
            "Epoch 4/15\n",
            "  9/150 [>.............................] - ETA: 16:06 - loss: 1.6182 - accuracy: 0.3524"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fd28a37db810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# without augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Early stopping -> sistemata\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=7,\n",
        "    mode='auto',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = optimizers.Adam(learning_rate=1e-2),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# with augmentation \n",
        "# history = model.fit(train_augmented, epochs=5 , validation_data=val , verbose = 1 , callbacks=[early_stopping] , class_weight = class_weights)\n",
        "\n",
        "# without augmentation \n",
        "history = model.fit(train, epochs=15 , validation_data=val ,steps_per_epoch=150, verbose = 1 , callbacks=[early_stopping] , class_weight = class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBYtrY-HeUMW"
      },
      "outputs": [],
      "source": [
        "#evaluation with history\n",
        "model.save('/content/drive/MyDrive/model/efficent_featurereuse.h5')   # always save your weights after training or during training\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4HtRfUuEBP4"
      },
      "outputs": [],
      "source": [
        "#disconnessione per non sprecare risorse\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_5O8b_ADTvn"
      },
      "source": [
        "### **OPTION 2 = fine tuning training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQjTTI9EeWIo"
      },
      "outputs": [],
      "source": [
        "# Freezing all layers until the fifth from the last\n",
        "model_ENB0.trainable = True\n",
        "for layer in model_ENB0.layers[:-5]:\n",
        " layer.trainable = False\n",
        "\n",
        " \n",
        "# model_ENB0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXqSsgjVe_2R",
        "outputId": "e14b6c30-823a-48ee-d701-ef24bd4b7688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "100/100 [==============================] - 1766s 18s/step - loss: 1.6115 - accuracy: 0.3122 - val_loss: 8.1276 - val_accuracy: 0.2500\n",
            "Epoch 2/20\n",
            "  2/100 [..............................] - ETA: 40:16 - loss: 1.5369 - accuracy: 0.3125"
          ]
        }
      ],
      "source": [
        "#Early stopping -> sistemata\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=7,\n",
        "    mode='auto',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Pre Training\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = optimizers.Adam(learning_rate=1e-2),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "# epochs = 20  # @param {type: \"slider\", min:10, max:100}\n",
        "\n",
        "#history = model.fit(train_augmented, epochs=epochs ,steps_per_epoch=100, validation_data=val , verbose = 1 , callbacks=[early_stopping] , class_weight = class_weights)\n",
        "history = model.fit(train, epochs=epochs ,steps_per_epoch=100, validation_data=val , verbose = 1 , callbacks=[early_stopping] , class_weight = class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dw05uscfGlp"
      },
      "outputs": [],
      "source": [
        "#evaluation with history\n",
        "model.save('/content/drive/MyDrive/model/efficent_finetuning.h5')   # always save your weights after training or during training\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y09zXDsazxhV"
      },
      "outputs": [],
      "source": [
        "#disconnessione per non sprecare risorse\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMZ_W8ypiX69"
      },
      "source": [
        "# **4. INCEPTION V3 pre-trained**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV0zFR5vlLsE"
      },
      "source": [
        "[esempio](https://github.com/tejanirla/image_classification/blob/master/transfer_learning.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_jO3HQORN-h",
        "outputId": "e7d2dc21-fa0d-48e0-c779-caebc1822ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# setup directory used in the project for training and test phase\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/dataset/OCT/'\n",
        "train_dir = '/content/drive/MyDrive/dataset/OCT/train'\n",
        "test_dir = '/content/drive/MyDrive/dataset/OCT/test'\n",
        "val_dir = '/content/drive/MyDrive/dataset/OCT/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "51GwObqIROKa",
        "outputId": "baddd015-0e40-4fc5-f468-11c875ed04c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 83493 files belonging to 4 classes.\n",
            "Found 32 files belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\naug = ImageDataGenerator(\\n    rotation_range=10,\\n    zoom_range=0.15,\\n    width_shift_range=0.1,\\n    height_shift_range=0.1,\\n    shear_range=0.15,\\n    horizontal_flip=False,\\n    vertical_flip=False,\\n    fill_mode=\"nearest\")\\n\\nimg_dim = 150\\nbatch_size=64\\ntrain_augmented = aug.flow_from_directory(\\n    directory=train_dir,\\n    target_size=(img_dim, img_dim),\\n    batch_size=batch_size ,\\n    shuffle=True,\\n    color_mode=\\'rgb\\',\\n    seed=None,\\n    class_mode=\\'categorical\\')\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#Augmentation --> farlo con il layer\n",
        "\n",
        "m_channels = 3\n",
        "train = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb', #--> attenzione a questo valore , con efficentnet per forza rgb\n",
        "    batch_size=64,\n",
        "    image_size=(150, 150),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n",
        "val = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='rgb',\n",
        "    batch_size=64,\n",
        "    image_size=(150, 150),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n",
        "'''\n",
        "\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    fill_mode=\"nearest\")\n",
        "\n",
        "img_dim = 150\n",
        "batch_size=64\n",
        "train_augmented = aug.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(img_dim, img_dim),\n",
        "    batch_size=batch_size ,\n",
        "    shuffle=True,\n",
        "    color_mode='rgb',\n",
        "    seed=None,\n",
        "    class_mode='categorical')\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhbXgAfnRnP0",
        "outputId": "500a9c79-3c97-43d6-c2c1-d63111a0a169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "#usato per la normalizzazzione -> lo fa keras negli esempi -> velocizza il training\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)    \n",
        "val = val.map(lambda x, y: (normalization_layer(x), y))\n",
        "train = train.map(lambda x, y: (normalization_layer(x), y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSKrzDOWRnrQ"
      },
      "outputs": [],
      "source": [
        "# mantiene le immagini in memoria dopo che sono state caricate dal disco durante la prima epoca. Velocizza il training\n",
        "#lo fa keras negli esempi\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val = val.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dswVsuBwQsPw"
      },
      "outputs": [],
      "source": [
        "# Import the inception model  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VffUQqasQyBG",
        "outputId": "e258bd92-fa17-484d-d93d-bd6b96a0582c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), # Shape of our images\n",
        "                                include_top = False, # Leave out the last fully connected layer\n",
        "                                weights = 'imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca6rUaI7Q0P7"
      },
      "outputs": [],
      "source": [
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3kIwpzWn7Kd"
      },
      "outputs": [],
      "source": [
        "#Early stopping -> sistemata\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='loss', \n",
        "    verbose=1,\n",
        "    patience=7,\n",
        "    mode='auto',\n",
        "    restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faXfrRCxhfrL"
      },
      "source": [
        "### **Feature reuse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQWowPybhgo4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(pre_trained_model.output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (4, activation='softmax')(x)           \n",
        "\n",
        "model = Model( pre_trained_model.input, x) \n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(optimizer = adam, \n",
        "              loss = 'categorical_crossentropy', \n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_gtVvy9hqIv",
        "outputId": "53152252-b68b-4e7f-e3c3-9518734c1e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "400/400 [==============================] - 3035s 7s/step - loss: 10.5843 - accuracy: 0.7229 - val_loss: 0.7223 - val_accuracy: 0.5938\n",
            "Epoch 2/20\n",
            "400/400 [==============================] - 2436s 6s/step - loss: 0.6352 - accuracy: 0.7633 - val_loss: 0.5448 - val_accuracy: 0.8125\n",
            "Epoch 3/20\n",
            "400/400 [==============================] - 1823s 5s/step - loss: 0.6544 - accuracy: 0.7573 - val_loss: 0.6167 - val_accuracy: 0.6562\n",
            "Epoch 4/20\n",
            "105/400 [======>.......................] - ETA: 21:54 - loss: 0.8517 - accuracy: 0.7076"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8000 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r400/400 [==============================] - 470s 1s/step - loss: 0.8517 - accuracy: 0.7076 - val_loss: 0.9012 - val_accuracy: 0.5000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "history = model.fit(\n",
        "            train,\n",
        "            validation_data = val,\n",
        "            steps_per_epoch = 400,\n",
        "            epochs = 20,\n",
        "            verbose = 1,\n",
        "            callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEX7eE2Ih3da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "95b4fcaa-92a2-4b18-a259-06808db61a62"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "model.save('/content/drive/MyDrive/model/inceptionv3_featurereuse400step.h5')   # always save your weights after training or during training\n",
        "\n",
        "#evaluation with history\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYfZos3qhZoD"
      },
      "source": [
        "### **Fine Tuning** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__FK590qQ6KF",
        "outputId": "ae19d979-d981-4665-ae4a-1f1f717f1149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 7, 7, 768)\n"
          ]
        }
      ],
      "source": [
        "#unfreeze last layer\n",
        "last_layer = pre_trained_model.get_layer('mixed7') #Only use layers including and above 'mixed7'\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR4-uUL5RHy8",
        "outputId": "55a2c16e-8dbf-4ed5-f32a-1bbc38d320a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 7, 7, 768)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (4, activation='sigmoid')(x)           \n",
        "\n",
        "model = Model( pre_trained_model.input, x) \n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0HUgVjCRKGA",
        "outputId": "0a715faa-e385-4ab3-ef59-a21b3d0c6516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/23\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "history = model.fit(\n",
        "            train,\n",
        "            validation_data = val,\n",
        "            steps_per_epoch = 100,\n",
        "            epochs = 23,\n",
        "            validation_steps = 50,\n",
        "            verbose = 2,\n",
        "            callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbB21eGsRMkY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "model.save('/content/drive/MyDrive/model/inceptionv3_finetuning.h5')   # always save your weights after training or during training\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMXLQsaHDzVj"
      },
      "source": [
        "# **5. NEURAL NET from scratch - PROVA GIUSEPPE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns0ptuAshQFn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p64H9Bdnhh3p",
        "outputId": "f5b26751-59e2-4c70-875d-60d292b0aae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# setup directory used in the project for training and test phase\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/dataset/OCT/'\n",
        "train_dir = '/content/drive/MyDrive/dataset/OCT/train'\n",
        "test_dir = '/content/drive/MyDrive/dataset/OCT/test'\n",
        "val_dir = '/content/drive/MyDrive/dataset/OCT/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBWe4FSEhVEm"
      },
      "outputs": [],
      "source": [
        "# sarebbe l'augmentation\n",
        "image_gen = ImageDataGenerator(rotation_range=25,\n",
        "                              width_shift_range=0.1,\n",
        "                              height_shift_range=0.1,rescale=1/255,shear_range=0.2,\n",
        "                              zoom_range=0.2,horizontal_flip=True,fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SywX2Sjhpwr",
        "outputId": "27953209-a421-48a1-f4c2-c598462ef6bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 148, 148, 64)      640       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 148, 148, 64)     256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 74, 74, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 72, 72, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 72, 72, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 36, 36, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 36, 36, 128)       0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 34, 34, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 34, 34, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 17, 17, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 17, 17, 256)       0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 15, 15, 512)       1180160   \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 15, 15, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 7, 7, 512)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512)               12845568  \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,668,036\n",
            "Trainable params: 14,664,068\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(150, 150, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(lr=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE0myRQoiH8t",
        "outputId": "4ed04f3d-97c7-42cc-dff5-ed462218d1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 83493 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "training_image_aug =image_gen.flow_from_directory(train_dir , target_size=(150,150),batch_size=64 , color_mode='grayscale')\n",
        "val_image_aug =image_gen.flow_from_directory(val_dir , target_size=(150,150),batch_size=64, color_mode='grayscale')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0bnQYzUpDWS"
      },
      "source": [
        "prova con gray scale e meno augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm2__ucZiWLJ",
        "outputId": "8748aa61-adc7-4325-fde1-859074ec1ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 83493 images belonging to 4 classes.\n",
            "Found 32 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "image_gen = ImageDataGenerator(rescale=1/255,shear_range=0,\n",
        "                              zoom_range=0,horizontal_flip=False,fill_mode='nearest')\n",
        "training_image_aug =image_gen.flow_from_directory(train_dir , target_size=(150,150),batch_size=64 , color_mode='grayscale')\n",
        "val_image_aug =image_gen.flow_from_directory(val_dir , target_size=(150,150),batch_size=64, color_mode='grayscale')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "RjDm2WQIig12",
        "outputId": "9ec8c080-b4e6-4caa-b548-f00053ae9712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9e88e537afd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(training_image,\n\u001b[0m\u001b[1;32m      2\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               validation_data=val_image)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Early stopping -> sistemata\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=7,\n",
        "    mode='auto',\n",
        "    restore_best_weights=True)\n",
        "history = model.fit(training_image,\n",
        "                              steps_per_epoch=200,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_image,\n",
        "                              callbacks=[early_stopping]\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XH3n8fQjmNG"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/model/Scrath3.h5')   # always save your weights after training or during training\n",
        "#evaluation with history\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqkJAD250RLg"
      },
      "source": [
        "# **STOP - EXECUTE ALWAYS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOe9Kl-J0WNJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq-_DOFZ0HXT"
      },
      "source": [
        "# **Analisi risultati migliori su kaggle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCLnvzs60MWZ"
      },
      "source": [
        "\n",
        "\n",
        "1.   vgg16 - train 89%   [link](https://www.kaggle.com/code/carloalbertobarbano/vgg16-transfer-learning-pytorch)\n",
        "\n",
        "2.    vgg16 - train 91%  &  inceptionv3 - train 92 [link](https://www.kaggle.com/code/paultimothymooney/detect-retina-damage-from-oct-images)\n",
        "\n",
        "3.  custom - train 97% [link](https://www.kaggle.com/code/chir0313/oct-retinal-damage-prediction-99-accuracy)\n",
        "\n",
        "4. resnet - train 99% [ link](https://www.kaggle.com/code/rashaadmeyer/retinal-oct-pytorch-99-8-resnet18)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK51T5YK4Xmy"
      },
      "source": [
        "## Prova 3. CUSTOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiwFHfx34uJ2"
      },
      "outputs": [],
      "source": [
        "# setup directory used in the project for training and test phase\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/dataset/OCT/'\n",
        "train_dir = '/content/drive/MyDrive/dataset/OCT/train'\n",
        "test_dir = '/content/drive/MyDrive/dataset/OCT/test'\n",
        "val_dir = '/content/drive/MyDrive/dataset/OCT/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nydJiok_4zzN",
        "outputId": "b8fa88ce-e6f0-4816-e765-2908fa0e8567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 83493 files belonging to 4 classes.\n",
            "Found 32 files belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "#Augmentation --> farlo con il layer\n",
        "\n",
        "m_channels = 3\n",
        "train = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='grayscale', #--> attenzione a questo valore , con efficentnet per forza rgb\n",
        "    batch_size=64,\n",
        "    image_size=(150, 150),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n",
        "val = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=None,\n",
        "    color_mode='grayscale',\n",
        "    batch_size=64,\n",
        "    image_size=(150, 150),\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    validation_split=None,\n",
        "    subset=None,\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_Db-WZj412o",
        "outputId": "d587dc4f-eacf-4f76-f41b-de20f357e9c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "#usato per la normalizzazzione -> lo fa keras negli esempi -> velocizza il training\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)    \n",
        "val = val.map(lambda x, y: (normalization_layer(x), y))\n",
        "train = train.map(lambda x, y: (normalization_layer(x), y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nboXIilp4d3i",
        "outputId": "e57a69f9-2112-43a6-945c-985f09da6808"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_24 (Conv2D)          (None, 146, 146, 60)      1560      \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 142, 142, 60)      90060     \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 139, 139, 30)      28830     \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 136, 136, 30)      14430     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 68, 68, 30)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 68, 68, 30)        0         \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 64, 64, 60)        45060     \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 61, 61, 30)        28830     \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 58, 58, 30)        14430     \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 29, 29, 30)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 29, 29, 30)        0         \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 26, 26, 60)        28860     \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 13, 13, 60)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 11, 11, 30)        16230     \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 9, 9, 30)          8130      \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 4, 4, 30)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 4, 4, 30)          0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 480)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5000)              2405000   \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 5000)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4)                 20004     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,701,424\n",
            "Trainable params: 2,701,424\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
        "\n",
        "#creo il modello\n",
        "def model():\n",
        "  filters=60\n",
        "  sizeoffilter1 = (5,5)\n",
        "  sizeoffilter2 = (4,4)\n",
        "  sizeoffilter3 = (3,3)\n",
        "  sizeofpool = (2,2)\n",
        "  node=5000\n",
        "\n",
        "  model = Sequential();\n",
        "  model.add((Conv2D(filters,sizeoffilter1,input_shape=(150,150,1)\n",
        "  ,activation=\"relu\")))\n",
        "\n",
        "  model.add((Conv2D(filters,sizeoffilter1,activation=\"relu\")))\n",
        "  model.add((Conv2D(filters//2,sizeoffilter2,activation=\"relu\")))\n",
        "  model.add((Conv2D(filters//2,sizeoffilter2,activation=\"relu\")))\n",
        "  model.add(MaxPooling2D(pool_size=sizeofpool))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add((Conv2D(filters,sizeoffilter1,activation=\"relu\")))\n",
        "  model.add((Conv2D(filters//2,sizeoffilter2,activation=\"relu\")))\n",
        "  model.add((Conv2D(filters//2,sizeoffilter2,activation=\"relu\")))\n",
        "  model.add(MaxPooling2D(pool_size=sizeofpool))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add((Conv2D(filters,sizeoffilter2,activation=\"relu\")))\n",
        "  model.add(MaxPooling2D(pool_size=sizeofpool))\n",
        "  model.add((Conv2D(filters//2,sizeoffilter3,activation=\"relu\")))\n",
        "  model.add((Conv2D(filters//2,sizeoffilter3,activation=\"relu\")))\n",
        "  model.add(MaxPooling2D(pool_size=sizeofpool))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(node,activation=\"relu\"))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(4,activation=\"softmax\"))\n",
        "\n",
        "  model.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "  \n",
        "model=model()\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vefLhATM4k1h"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "hEuGTPwv4mRo",
        "outputId": "e7c038bc-8919-40c3-c950-d344ccc86964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "  1/150 [..............................] - ETA: 1:54:49 - loss: 2.5559 - accuracy: 0.4219"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9a4a317cdde6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     restore_best_weights=True)\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#steps_per_epoch=150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m history = model.fit(train,epochs=30,validation_data =val ,batch_size=256,\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Early stopping -> sistemata\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=7,\n",
        "    mode='auto',\n",
        "    restore_best_weights=True)\n",
        "#steps_per_epoch=150\n",
        "history = model.fit(train,epochs=30,validation_data =val ,batch_size=256,\n",
        "                    steps_per_epoch=150,\n",
        "                    shuffle=True,\n",
        "                    max_queue_size=20,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=1,\n",
        "                    verbose = 1 ,\n",
        "                   callbacks=[early_stopping] , \n",
        "                    class_weight = class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **USE THE MODEL**"
      ],
      "metadata": {
        "id": "n3j8N-6mcAyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model\n",
        "from keras.models import load_model\n",
        "model = load_model('my_model.h5')"
      ],
      "metadata": {
        "id": "ceGwA4o7cD7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(optimizer = adam, \n",
        "              loss = 'categorical_crossentropy', \n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "1-ncY9pEckPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u_xiT628crwJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iz2u4wsGRv6x",
        "oOhAx5JqRgm1",
        "1_-tCbAQ6jrV",
        "CZoh6L2MUD93",
        "q_5O8b_ADTvn",
        "mK51T5YK4Xmy"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNbCAurhi7MXc6GU+dSIVqB",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
